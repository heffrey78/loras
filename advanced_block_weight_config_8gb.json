{
    "LoRA_type": "Standard",
    "additional_parameters": "",
    "block_alphas": "2,2,2,4,4,4,4,8,8,8,8,16,16,16,8,8,8,8,4,4,4,4,2,2,2",
    "block_dims": "2,2,2,4,4,4,4,8,8,8,8,16,16,16,8,8,8,8,4,4,4,4,2,2,2",
    "block_lr_zero_threshold": "0.001",
    "bucket_no_upscale": false,
    "bucket_reso_steps": 64.0,
    "cache_latents": true,
    "caption_dropout_every_n_epochs": 2.0,
    "caption_dropout_rate": 0.05,
    "caption_extension": ".txt",
    "caption_tag_dropout_rate": 0.1,
    "clip_skip": 2,
    "color_aug": false,
    "conv_alpha": 1,
    "conv_alphas": "",
    "conv_dim": 1,
    "conv_dims": "",
    "down_lr_weight": "1,0.5,0.5,0.5,1,1,1,1,1,1,1",
    "enable_bucket": true,
    "epoch": 12,
    "flip_aug": false,
    "full_fp16": false,
    "gradient_accumulation_steps": 1.0,
    "gradient_checkpointing": true,
    "keep_tokens": "2",
    "learning_rate": "5e-05",
    "lora_network_weights": "",
    "lr_scheduler": "polynomial",
    "lr_scheduler_num_cycles": "",
    "lr_scheduler_power": "2.0",
    "lr_warmup": "100",
    "max_bucket_reso": 960,
    "max_data_loader_n_workers": "1",
    "max_resolution": "768,768",
    "max_token_length": "225",
    "max_train_epochs": "",
    "mem_eff_attn": false,
    "mid_lr_weight": "1",
    "min_bucket_reso": 512,
    "min_snr_gamma": 5,
    "mixed_precision": "fp16",
    "multires_noise_discount": 0.2,
    "multires_noise_iterations": 4,
    "network_alpha": 32,
    "network_dim": 32,
    "no_token_padding": false,
    "noise_offset": "0.06",
    "num_cpu_threads_per_process": 2,
    "optimizer": "AdamW8bit",
    "optimizer_args": "",
    "persistent_data_loader_workers": false,
    "prior_loss_weight": 1.0,
    "random_crop": false,
    "save_every_n_epochs": 3,
    "save_precision": "fp16",
    "scale_v_pred_loss_like_noise_pred": false,
    "seed": "1337",
    "shuffle_caption": true,
    "stop_text_encoder_training": 8,
    "text_encoder_lr": "2e-05",
    "train_batch_size": 1,
    "training_comment": "Block-weighted LoRA focusing on middle layers for style/concept",
    "unet_lr": "5e-05",
    "up_lr_weight": "1,1,1,1,1,1,1,1,0.5,0.5,0.5,1",
    "v2": false,
    "v_parameterization": false,
    "vae_batch_size": 0,
    "xformers": true
}